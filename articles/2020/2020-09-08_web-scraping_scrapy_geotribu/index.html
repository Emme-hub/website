<!DOCTYPE html><html lang=en> <head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=description content="Site indépendant de veille sur la géomatique libre et open source. Articles, tutoriels et revues de presse (#GeoRDP) sur l'information géographique, les SIG, la cartographie, la représentation des données..."><meta name=author content=Geotribu><link href=https://static.geotribu.fr/articles/2020/2020-09-08_web-scraping_scrapy_geotribu/ rel=canonical><link rel="shortcut icon" href=../../../img/favicon.ico><title>Le web-scraping avec Scrapy - Geotribu</title><link href=../../../css/bootstrap.min.css rel=stylesheet><link href=../../../css/font-awesome.min.css rel=stylesheet><link href=../../../css/bootstrap-theme.min.css rel=stylesheet><link href=../../../../../../theme/assets/stylesheets/extra.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/wa-mediabox@1.0.1/dist/wa-mediabox.min.css rel=stylesheet><!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.3/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]--><script src=../../../js/jquery-1.10.2.min.js></script><script src=../../../js/bootstrap.min.js></script><script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'None', 'auto');
            ga('send', 'pageview');
        </script></head> <body> <div class="navbar navbar-default navbar-fixed-top" role=navigation> <div class=container> <div class=navbar-header> <button type=button class=navbar-toggle data-toggle=collapse data-target=.navbar-collapse> <span class=sr-only>Toggle navigation</span> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href=../../..>Geotribu</a> </div> <div class="navbar-collapse collapse"> <ul class="nav navbar-nav"> <li> <a href=../../..>&#127968; Accueil</a> </li> <li class=dropdown> <a href=# class=dropdown-toggle data-toggle=dropdown>Revues de presse <b class=caret></b></a> <ul class=dropdown-menu> <li> <a href=../../..>2021</a> </li> <li> <a href=../../..>2020</a> </li> <li> <a href=../../..>2017</a> </li> <li> <a href=../../..>2016</a> </li> <li> <a href=../../..>2015</a> </li> <li> <a href=../../..>2014</a> </li> <li> <a href=../../..>2013</a> </li> <li> <a href=../../..>2012</a> </li> <li> <a href=../../..>2011</a> </li> <li> <a href=../../..>2010</a> </li> </ul> </li> <li class="dropdown active"> <a href=# class=dropdown-toggle data-toggle=dropdown>Articles <b class=caret></b></a> <ul class=dropdown-menu> <li> <a href=../../..>2021</a> </li> <li class=active> <a href=../../..>2020</a> </li> <li> <a href=../../..>2015</a> </li> <li> <a href=../../..>2014</a> </li> <li> <a href=../../..>2013</a> </li> <li> <a href=../../..>2012</a> </li> <li> <a href=../../..>2011</a> </li> <li> <a href=../../..>2010</a> </li> <li> <a href=../../..>2009</a> </li> <li> <a href=../../..>2008</a> </li> </ul> </li> <li class=dropdown> <a href=# class=dropdown-toggle data-toggle=dropdown>Contribuer <b class=caret></b></a> <ul class=dropdown-menu> <li> <a href=../../../contribuer/introduction/ >Contribuer à Geotribu</a> </li> <li> <a href=../../../contribuer/requirements/ >Prérequis pour la contribution</a> </li> <li> <a href=../../../contribuer/contribution_guide/ >Guide de contribution</a> </li> <li> <a href=../../..>Editer le contenu</a> </li> <li> <a href=../../..>Générer le site web</a> </li> <li> <a href=../../..>Guides de rédaction</a> </li> <li> <a href=../../..>Archives</a> </li> </ul> </li> <li class=dropdown> <a href=# class=dropdown-toggle data-toggle=dropdown>A propos <b class=caret></b></a> <ul class=dropdown-menu> <li> <a href=../../../team/ >L'équipe Geotribu</a> </li> <li> <a href=../../../team/acha/ >Aurélien Chaumet</a> </li> <li> <a href=../../../team/avdc/ >Arnaud Vandecasteele</a> </li> <li> <a href=../../../team/avha/ >Adrien Van Hamme</a> </li> <li> <a href=../../../team/edel/ >Etienne Delay</a> </li> <li> <a href=../../../team/fbor/ >Florian Boret</a> </li> <li> <a href=../../../team/fgob/ >Fabien Goblet</a> </li> <li> <a href=../../../team/gdbo/ >Guillaume de Boyer</a> </li> <li> <a href=../../../team/jmou/ >Julien Moura</a> </li> <li> <a href=../../../team/jory/ >Jérémie Ory</a> </li> <li> <a href=../../../team/jpie/ >Julie Pierson</a> </li> <li> <a href=../../../team/mraj/ >Mathieu Rajerison</a> </li> <li> <a href=../../../team/pver/ >Pierre Vernier</a> </li> <li> <a href=../../../team/rbov/ >Rémi Bovard</a> </li> <li> <a href=../../../team/rqui/ >Rodolphe Quiédeville</a> </li> <li> <a href=../../../team/tgra/ >Thomas Gratier</a> </li> <li> <a href=../../..>Archives</a> </li> </ul> </li> </ul> <ul class="nav navbar-nav navbar-right"> <li> <a href=../2020-09-09_geodatadays/ rel=next> <i class="fa fa-arrow-left"></i> Previous </a> </li> <li> <a href=../2020-08-31_geotribu_histoire/ rel=prev> Next <i class="fa fa-arrow-right"></i> </a> </li> <li> <a href=https://github.com/geotribu/website/ > geotribu/website </a> </li> </ul> </div> </div> </div> <div class=container> <div class=row> <div class=col-md-3><div class="bs-sidebar hidden-print affix well" role=complementary> <ul class="nav bs-sidenav"> <li class="main active"><a href=#recuperer-les-anciens-contenus-le-web-scraping-a-la-rescousse>Récupérer les anciens contenus : le web-scraping à la rescousse</a></li> <li><a href=#introduction>Introduction</a></li> <li><a href=#le-web-scraping-cest-quoi>Le web-scraping, c'est quoi</a></li> <li><a href=#aspirons-lancien-geotribu>Aspirons l'ancien Geotribu</a></li> <li><a href=#conclusion>Conclusion</a></li> <li><a href=#auteur>Auteur</a></li> <li class="main "><a href=#julien-moura>Julien Moura</a></li> </ul> </div></div> <div class=col-md-9 role=main> <h1 id=recuperer-les-anciens-contenus-le-web-scraping-a-la-rescousse>Récupérer les anciens contenus : le web-scraping à la rescousse<a class=headerlink href=#recuperer-les-anciens-contenus-le-web-scraping-a-la-rescousse title="Permanent link">#</a></h1> <p>:calendar: Date de publication initiale : 7 septembre 2020</p> <p><strong>Mots-clés :</strong> Geotribu | histoire | Scrapy | Python | web-scraping</p> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">#</a></h2> <p>Après avoir disserté sur <a href=../2020-08-31_geotribu_histoire/ >la petite histoire de Geotribu</a>, il est désormais temps de se pencher sur la méthode retenue pour récupérer les anciens contenus.</p> <p>Pendant un temps, on a eu l'espoir de remonter le site depuis une ancienne sauvegarde, quitte à faire le deuil des contenus les plus récents. Techniquement faisable, l'idée de repartir sur un Drupal vieillot et difficile à maintenir n'a pas séduit grand monde.</p> <p>Lorsque que le moment est venu, j'ai donc opté pour le web-scraping. L'occasion ici de partager mon expérience, comme d'habitude.</p> <h2 id=le-web-scraping-cest-quoi>Le web-scraping, c'est quoi<a class=headerlink href=#le-web-scraping-cest-quoi title="Permanent link">#</a></h2> <p>C'est une technique visant à aspirer le contenu d'un site web pour le stocker dans un format pivot en vue d'une réutilisation, prévue ou non par l'éditeur initial. Pour plus de détails, <a href=https://fr.wikipedia.org/wiki/Web_scraping>la page Wikipédia sera certainement plus complète</a>.</p> <p>Si le nom ne vous dit rien, c'est pourtant une technique qui est largement utilisée de façon sous-jacente à de nombreux services : référencement automatisé des sites par les moteurs de recherche, tests automatisés d'applications webs (<em>headless</em>), comparateurs de prix, agrégateurs de contenus (actualités par exemple) etc.</p> <p>Le fonctionnement global est schématisé de la façon suivante :</p> <p><a data-mediabox=scraping data-title="Le web-scraping schématisé. Crédits Web Harvy" href=https://cdn.geotribu.fr/img/tuto/webscraping/web_scraping.png><img alt="web scraping schéma" class=img-center loading=lazy src=https://cdn.geotribu.fr/img/tuto/webscraping/web_scraping.png title="Le web-scraping schématisé. Crédits : Web Harvy"></a></p> <hr> <h2 id=aspirons-lancien-geotribu>Aspirons l'ancien Geotribu<a class=headerlink href=#aspirons-lancien-geotribu title="Permanent link">#</a></h2> <p>:cup_with_straw: Si on en croit cette simplification, on a donc besoin de 3 ingrédients :</p> <ul> <li>un site web qui tourne</li> <li>un logiciel (ou un service) de web-scraping</li> <li>un/des formats de destination</li> </ul> <h3 id=le-site-local-et-internet-archive>Le site : local et Internet Archive<a class=headerlink href=#le-site-local-et-internet-archive title="Permanent link">#</a></h3> <p>Pour ce premier point, je suis d'abord parti de la dernière sauvegarde dont disposait Fabien mais qui datait. Après quelques batailles homme-machine, je gagnai la guerre et déployai l'ancien Geotribu en local. Pour l'anecdote, travaillant alors sur Windows, j'ai même eu le toupet de faire tourner le site sur <a href=https://fr.wikipedia.org/wiki/WampServer>WAMP</a> !</p> <p>Pour les contenus les plus récents, je me suis tourné vers l'<a href=https://archive.org>Internet Archive</a> sur les conseils de <a href=https://www.linkedin.com/in/vincentpicavet/ >Vincent Picavet</a> (merci à lui !), d'où j'ai tiré les captures d'écran du précédent article.</p> <p>Le projet permet bien un accès via des <a href=https://archive.org/services/docs/api/ >APIs REST</a> et ne se prête pas trop au web-scraping intensif pour qui souhaite respecter le <a href=https://fr.wikipedia.org/wiki/Fair_use><em>fair-use</em></a>, mais cela aurait demandé de faire du développement spécifique et <em>one-shot</em> qui plus est.</p> <p>L'une des difficultés étant que l'archivage (qui utilise lui-même du web-scraping) n'est ni régulier, ni exhaustif ; un contenu étant donc potentiellement absent ou présent selon les dates d'archives. Après quelques manipulations, j'ai donc retenu : <a href=https://web.archive.org/web/20170222060359/http://www.geotribu.net/ >l'archive du 22 février 2017</a>.</p> <p><a data-mediabox=scraping data-title="Serveurs de l'Internet Archive" href=https://cdn.geotribu.fr/img/tuto/webscraping/internet_archive_server.jpg><img alt="Internet Archive serveurs" class=img-center loading=lazy src=https://cdn.geotribu.fr/img/tuto/webscraping/internet_archive_server.jpg title="Serveurs de l'Internet Archive"></a></p> <h3 id=le-logiciel-scrapy>Le logiciel : Scrapy<a class=headerlink href=#le-logiciel-scrapy title="Permanent link">#</a></h3> <p>Python étant à la fois mon langage de prédilection et l'un de ceux tout indiqués pour le web-scraping, j'ai donc opté pour <a href=https://scrapy.org/ >Scrapy</a>. En avant pour l'installation dans un joli environnement virtuel avec Python 3.8 :</p> <div class=highlight><pre><span></span><code>python -m pip install <span class=nv>scrapy</span><span class=o>==</span><span class=m>2</span>.3.*
</code></pre></div> <p>Une fois installé, Scrapy permet de générer rapidement une structure de projet et aussi un shell interactif qui permet de "jouer" avec le site web visé : <code>scrapy shell</code>.</p> <h4 id=scrapy-shell>Scrapy shell<a class=headerlink href=#scrapy-shell title="Permanent link">#</a></h4> <p>Un site web c'est un ensemble plus ou moins organisé de pages parmi lesquelles on navigue à la souris ou au clavier. La première étape du web-scraping est donc de reproduire ce comportement sans interaction : naviguer dans le site web. Dans notre cas, on souhaite savoir parcourir la liste des revues de presse et déterminer comment on "ouvre" une revue de presse.</p> <p>On passe donc notre URL cible au Scrapy shell :</p> <div class=highlight><pre><span></span><code>scrapy shell <span class=s2>&quot;https://web.archive.org/web/20170222025421/http://www.geotribu.net/revues-de-presse&quot;</span>
</code></pre></div> <p>Scrapy réalise alors déjà un gros travail de requêtes, mise en cache, etc. qui se traduit par un bon paquet de messages. La page est stockée dans un objet attaché à la variable <code>response</code> :</p> <div class=highlight><pre><span></span><code><span class=o>&gt;&gt;&gt;</span> <span class=n>response</span>
<span class=o>&lt;</span><span class=mi>200</span> <span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>web</span><span class=o>.</span><span class=n>archive</span><span class=o>.</span><span class=n>org</span><span class=o>/</span><span class=n>web</span><span class=o>/</span><span class=mi>20170222025421</span><span class=o>/</span><span class=n>http</span><span class=p>:</span><span class=o>//</span><span class=n>www</span><span class=o>.</span><span class=n>geotribu</span><span class=o>.</span><span class=n>net</span><span class=o>/</span><span class=n>revues</span><span class=o>-</span><span class=n>de</span><span class=o>-</span><span class=n>presse</span><span class=o>&gt;</span>

<span class=o>&gt;&gt;&gt;</span> <span class=nb>dir</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
<span class=p>[</span><span class=o>...</span> <span class=s1>&#39;body&#39;</span><span class=p>,</span> <span class=s1>&#39;body_as_unicode&#39;</span><span class=p>,</span> <span class=s1>&#39;cb_kwargs&#39;</span><span class=p>,</span> <span class=s1>&#39;certificate&#39;</span><span class=p>,</span> <span class=s1>&#39;copy&#39;</span><span class=p>,</span> <span class=s1>&#39;css&#39;</span><span class=p>,</span> <span class=s1>&#39;encoding&#39;</span><span class=p>,</span> <span class=s1>&#39;flags&#39;</span><span class=p>,</span> <span class=s1>&#39;follow&#39;</span><span class=p>,</span> <span class=s1>&#39;follow_all&#39;</span><span class=p>,</span> <span class=s1>&#39;headers&#39;</span><span class=p>,</span> <span class=s1>&#39;ip_address&#39;</span><span class=p>,</span> <span class=s1>&#39;json&#39;</span><span class=p>,</span> <span class=s1>&#39;meta&#39;</span><span class=p>,</span> <span class=s1>&#39;replace&#39;</span><span class=p>,</span> <span class=s1>&#39;request&#39;</span><span class=p>,</span> <span class=s1>&#39;selector&#39;</span><span class=p>,</span> <span class=s1>&#39;status&#39;</span><span class=p>,</span> <span class=s1>&#39;text&#39;</span><span class=p>,</span> <span class=s1>&#39;url&#39;</span><span class=p>,</span> <span class=s1>&#39;urljoin&#39;</span><span class=p>,</span> <span class=s1>&#39;xpath&#39;</span><span class=p>]</span>
</code></pre></div> <p>Parmi les méthodes de <code>response</code>, 2 en particulier nous intéressent pour se balader dans la structure de la page : <code>css</code> et <code>xpath</code>. Il s'agit des deux langages que Scrapy appelle les <code>selectors</code>. Le premier, CSS, est donc la façon dont une page est présentée/rendue ; le second, <a href=https://fr.wikipedia.org/wiki/XPath>XPath</a>, moins connu, est conçu spécifiquement pour se balader dans les documents XML et consorts (donc HTML par extension).</p> <p>Si j'ai déjà eu à faire à XPath (ô joie des métadonnées mal formatées...), il est plus facile de repérer la structure cible avec le CSS. Surtout quand on a contribué au site web cible.</p> <h4 id=parcourir-le-site-web>Parcourir le site web<a class=headerlink href=#parcourir-le-site-web title="Permanent link">#</a></h4> <p>On garde en tête notre objectif : parcourir la page des revues de presse pour extraire les informations de chaque revue de presse à explorer séparément par la suite. Pour y parvenir, pas de secret : il faut identifier et suffisamment discriminer les styles CSS souhaités. Et Drupal 6 ne nous a pas vraiment préparé à cela...</p> <p><a data-mediabox=scraping data-title="La cascade de styles dans l'ancien site : un passage obligé" href=https://cdn.geotribu.fr/img/tuto/webscraping/scraping_geotribu_css.png><img alt="Sources Geotribu Drupal" class=img-center loading=lazy src=https://cdn.geotribu.fr/img/tuto/webscraping/scraping_geotribu_css.png title="La cascade de styles dans l'ancien site : un passage obligé"></a></p> <p>Après quelques litres de collyre en lisant le HTML/CSS et les messages d'erreur de Scrapy, voici ce à quoi on arrive :</p> <div class=highlight><pre><span></span><code><span class=c1># titre de la page</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>response</span><span class=o>.</span><span class=n>css</span><span class=p>(</span><span class=s1>&#39;title::text&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>getall</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span>
<span class=s1>&#39;GeoTribu | Revues de presse&#39;</span>

<span class=c1># première rdp de la liste</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>t</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>css</span><span class=p>(</span><span class=s1>&#39;div.title-and-meta&#39;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>

<span class=c1># titre</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>rdp_title_section</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>css</span><span class=p>(</span><span class=s2>&quot;div.title-and-meta&quot;</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>rdp_title</span> <span class=o>=</span> <span class=n>rdp_title_section</span><span class=o>.</span><span class=n>css</span><span class=p>(</span><span class=s2>&quot;h2.node__title a::text&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>rdp_title</span>
<span class=s1>&#39;Revue de presse du 27 janvier&#39;</span>

<span class=c1># url</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>rdp_url_rel</span> <span class=o>=</span> <span class=n>rdp_title_section</span><span class=o>.</span><span class=n>css</span><span class=p>(</span><span class=s2>&quot;h2.node__title a::attr(href)&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>rdp_url_rel</span>
<span class=s1>&#39;/web/20170222025421/http://www.geotribu.net/GeoRDP/20170127&#39;</span>
</code></pre></div> <p>On a donc compris comment isoler l'url relative de chaque revue de presse :v: !</p> <h4 id=decortiquer-les-contenus>Décortiquer les contenus<a class=headerlink href=#decortiquer-les-contenus title="Permanent link">#</a></h4> <p>Une fois la navigation résolue, on peut alors procéder de même avec le contenu des pages ciblées :</p> <div class=highlight><pre><span></span><code><span class=c1># -- Parcourir la revue de presse choisie</span>
<span class=n>fetch</span><span class=p>(</span><span class=s2>&quot;https://web.archive.org/&quot;</span> <span class=o>+</span> <span class=n>rdp_url_rel</span><span class=p>)</span>

<span class=c1># contenu de la rdp</span>
<span class=n>rdp</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>css</span><span class=p>(</span><span class=s1>&#39;article&#39;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>

<span class=c1># sections</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>rdp_sections</span>
<span class=p>[</span><span class=s1>&#39;Client&#39;</span><span class=p>,</span> <span class=s1>&#39;Serveur&#39;</span><span class=p>,</span> <span class=s1>&#39;Représentation Cartographique&#39;</span><span class=p>,</span> <span class=s1>&#39;Conférences&#39;</span><span class=p>,</span> <span class=s1>&#39;Divers&#39;</span><span class=p>]</span>
</code></pre></div> <p>Inutile de détailler davantage, je pense que tout le monde a compris le principe et cet article est déjà bien long !</p> <h4 id=mise-en-musique>Mise en musique<a class=headerlink href=#mise-en-musique title="Permanent link">#</a></h4> <p>Une fois que l'on a démêlé la structure des contenus ciblés, on met tout cela en musique dans le projet Scrapy :</p> <ul> <li>à chaque type de contenu, son <em>item</em> : un objet dont les attributs correspondent à ce que l'on attend de récupérer d'un contenu (titre, introduction, etc.)</li> <li>un <em>spider</em> chargé d'extraire les informations ciblées dans un ensemble de pages : par exemple, les articles et les revues de presse n'ont pas la même structure, il faut donc adapter le <em>crawling</em>. De plus, cela permet de cibler un certain type de contenus</li> <li>concevoir les <em>pipeline</em> qui traiteront les données récuprées : nettoyage des balises, exclusion, conversion dans le ou les formats de destination, etc.</li> <li>éventuellement jouer avec les couches intermédiaires (<em>middlewares</em>) pour gérer des cas particuliers ou personnaliser des comportements : redimensionnement des images, etc.</li> <li>le tout orchestré par un fichier de configuration <code>settings.py</code> dont les options sont nombreuses, sans compter les éventuelles extensions.</li> </ul> <p><a data-mediabox=scraping data-title="Schéma d'architecture des modules de Scrapy." href=https://cdn.geotribu.fr/img/tuto/webscraping/scrapy_architecture.png><img alt="Architecture Scrapy" class=img-center loading=lazy src=https://cdn.geotribu.fr/img/tuto/webscraping/scrapy_architecture.png title="Schéma d'architecture des modules de Scrapy."></a></p> <p>Bref, à ce stade, le travail est loin d'être terminé. Soyons réaliste : ça n'est pas très "rentable" lorsqu'il s'agit, comme dans notre cas, de faire tourner notre programme de scraping que quelques fois seulement, le temps en fait de le peaufiner et de gérer les cas particuliers.</p> <p>Mais l'idée est bien là : parcourir un site web de façon automatisée pour en extraire les contenus.</p> <h3 id=aller-plus-loin>Aller plus loin<a class=headerlink href=#aller-plus-loin title="Permanent link">#</a></h3> <p>Si vous souhaitez aller au bout de la démarche, vous pouvez lancer le projet que j'ai développé et utilisé pour Geotribu. Je n'ai pas la prétention de dire que c'est un modèle du genre mais ça a le mérite de tourner (sinon dites-le moi) et d'illustrer le propos :</p> <ul> <li><a href=https://github.com/geotribu/scraping_old_site/ >le code source</a></li> <li><a href=https://geotribu-web-scraping-resurrection.readthedocs.io/ >la documentation</a></li> </ul> <p>Une fois le projet installé, il est aisé de lancer le scraping via une simple ligne de commande, en spécifiant :</p> <div class=highlight><pre><span></span><code><span class=c1># pour les revues de presse</span>
scrapy crawl geotribu_rdp -L WARNING
<span class=c1># pour les articles</span>
scrapy crawl geotribu_articles -L WARNING
</code></pre></div> <hr> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">#</a></h2> <p>Les plus minutieux auront remarqué que tous les contenus n'ont pas été récupérés ou intégrés dans le site actuel. En effet, plusieurs raisons à cela.</p> <p>D'abord par manque de temps : par principe, je suis plus fervent d'investir dans le futur que dans le passé. Au-delà d'une certaine limite, autant consacrer du temps à des nouveaux contenus et à l'actualisation de la dynamique.</p> <p>Ensuite, cela fatigue les serveurs de l'<a href=https://archive.org>Internet Archive</a> : c'est pas très fair-play et de toute façon ça renvoie des <a href=https://developer.mozilla.org/fr/docs/Web/HTTP/Status/429>429</a> en chaîne. Il aurait fallu passer par des instances cloud permettant de simuler des connexions de différents endroits (<a href=https://www.scrapinghub.com/crawlera/ >Crawlera de ScrapingHub</a> par exemple), mais nous n'avons pas sassez de public ou de demande pour que le jeu en vaille la chandelle.</p> <p>Maintenant on sait comment moissonner un site web, en l'occurence l'ancien Geotribu. Dans le prochain article, on verra quoi faire de tous cette soupe de HTML/CSS/Javascript/Images.</p> <p align=middle><a class=md-button href=../2020-09-11_html2markdown>Suite : convertir des fichiers HTML en Markdown :fontawesome-solid-step-forward:</a></p> <hr> <h2 id=auteur>Auteur<a class=headerlink href=#auteur title="Permanent link">#</a></h2> <h1 id=julien-moura>Julien Moura<a class=headerlink href=#julien-moura title="Permanent link">#</a></h1> <p><img alt="Portrait Julien Moura" class=img-rdp-news-thumb src=https://cdn.geotribu.fr/img/internal/contributeurs/jmou.jfif title="Portrait Julien Moura"></p> <p>Géographe "sigiste" de formation, j'ai d'abord travaillé sur différentes thématiques et types de structures : la gestion des déchets en milieu urbain à Madagascar, le foncier d'intérêt général auprès de <a href=http://www.epf-reunion.com/ >l'EPF de La Réunion</a>, l'organisation et la résilience urbaine face aux risques naturels à Lima pour l'IRD.</p> <p>C'est en m'intéressant à la gouvernance et à l'ouverture des données géographiques que je travaille à <a href=https://www.isogeo.com>Isogeo</a> quelques années. L'occasion d'asseoir mes compétences en développement et gestion de produit informatique. En 2020, je deviens indépendant (In Geo Veritas) puis rejoins les rangs d'<a href=https://oslandia.com/ >Oslandia</a>.</p> <p>Féru des dynamiques de contributions, je participe activement à Geotribu depuis fin 2011 et, comme ça me manquait trop, j'ai décidé de lancer sa renaissance en 2020.</p> </div> </div> </div> <hr> <footer class=container> <center><span class=copyleft>&copy;</span> Geotribu</center> <center>Documentation built with <a href=http://www.mkdocs.org>MkDocs</a> and <a href=http://kristopolous.github.io/BOOTSTRA.386>BOOTSTRA.386</a>.</center> </footer> <script src=../../../../../../theme/assets/javascripts/extra.js></script> <script src=https://cdn.jsdelivr.net/npm/wa-mediabox@1.0.1/dist/wa-mediabox.min.js></script> <script src=https://unpkg.com/mermaid@8.6.4/dist/mermaid.min.js></script> <script src=../../../../../../search/main.js></script> </body> </html>